    PID TTY      STAT   TIME COMMAND
2261396 ?        Ssl    0:41 /opt/conda/envs/py_3.9/bin/python -u pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --context-parallel-size 2 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 14336 --num-attention-heads 32 --seq-length 2048 --max-position-embeddings 128000 --untie-embeddings-and-output-weights --position-embedding-type rope --no-position-embedding --disable-bias-linear --swiglu --init-method-std 0.02 --attention-dropout 0.0 --hidden-dropout 0.0 --normalization RMSNorm --micro-batch-size 7 --global-batch-size 112 --train-iters 10 --no-async-tensor-model-parallel-allreduce --bf16 --no-masked-softmax-fusion --disable-bias-linear --tokenizer-type HuggingFaceTokenizer --tokenizer-model ../checkpoint/llama3-8b --dataloader-type cyclic --save-interval 200000 --tensorboard-dir experiment/1nodes_rank0_train_8B_mbs7_bs112_tp1_pp1_cp2_iter10/nocompile1_TE_FP16_1/2024-09-18_00-06-29 --log-interval 1 --eval-interval 320000 --eval-iters 10 --num-workers 8 --mock-data --log-interval 1 --save-interval 5000 --log-throughput --no-save-optim --eval-iters -1 --group-query-attention --num-query-groups 8 --no-gradient-accumulation-fusion --distributed-backend nccl --distributed-timeout-minutes 30 --use-distributed-optimizer --overlap-param-gather --overlap-grad-reduce --use-flash-attn --sequence-parallel --use-mcore-models --transformer-impl=transformer_engine --fp8-margin=0 --fp8-format=hybrid --fp8-interval=1 --fp8-amax-history-len=1024 --fp8-amax-compute-algo=max --attention-softmax-in-fp32 --lr 1e-4 --min-lr 1e-5 --lr-decay-iters 320000 --lr-decay-style cosine --weight-decay 1.0e-1 --clip-grad 1.0 --optimizer sgd
