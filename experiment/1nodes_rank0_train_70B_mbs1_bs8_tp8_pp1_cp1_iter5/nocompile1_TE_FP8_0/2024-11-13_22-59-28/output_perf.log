W1113 22:59:31.659180 139685672710784 torch/distributed/run.py:778] 
W1113 22:59:31.659180 139685672710784 torch/distributed/run.py:778] *****************************************
W1113 22:59:31.659180 139685672710784 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 22:59:31.659180 139685672710784 torch/distributed/run.py:778] *****************************************
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,none}]
                       [--use-rotary-position-embeddings]
                       [--rotary-base ROTARY_BASE]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--use-rope-scaling] [--no-position-embedding]
                       [--disable-te-fused-rope]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--multi-latent-attention]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--decrease-batch-size-if-needed]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--use-pytorch-profiler]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--tp-comm-overlap-rs-dgrad]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--tp-comm-bootstrap-backend {nccl,mpi,gloo}]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--deterministic-mode]
                       [--check-weight-hash-across-dp-replicas-interval CHECK_WEIGHT_HASH_ACROSS_DP_REPLICAS_INTERVAL]
                       [--calculate-per-token-loss]
                       [--train-sync-interval TRAIN_SYNC_INTERVAL]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--cross-entropy-loss-fusion]
                       [--use-flash-attn] [--disable-bias-linear]
                       [--add-qkv-bias] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--no-gradient-accumulation-fusion]
                       [--use-mcore-models] [--use-legacy-models]
                       [--manual-gc] [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root,WSD}]
                       [--lr-wsd-decay-style {exponential,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-wsd-decay-samples LR_WSD_DECAY_SAMPLES]
                       [--lr-wsd-decay-iters LR_WSD_DECAY_ITERS]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng]
                       [--non-persistent-save-interval NON_PERSISTENT_SAVE_INTERVAL]
                       [--non-persistent-ckpt-type {global,local,in_memory,None}]
                       [--non-persistent-global-ckpt-dir NON_PERSISTENT_GLOBAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-dir NON_PERSISTENT_LOCAL_CKPT_DIR]
                       [--non-persistent-local-ckpt-algo {fully_parallel,atomic}]
                       [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format DIST_CKPT_FORMAT_DEPRECATED]
                       [--ckpt-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-format {torch,torch_dist,zarr}]
                       [--ckpt-convert-save CKPT_CONVERT_SAVE]
                       [--ckpt-convert-update-legacy-dist-opt-format]
                       [--ckpt-fully-parallel-save]
                       [--no-ckpt-fully-parallel-save] [--async-save]
                       [--ckpt-fully-parallel-load]
                       [--ckpt-assume-constant-structure]
                       [--dist-ckpt-strictness {assume_ok_unexpected,log_unexpected,log_all,raise_unexpected,raise_all,return_unexpected,return_all,ignore_all}]
                       [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--encoder-tensor-model-parallel-size ENCODER_TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--encoder-pipeline-model-parallel-size ENCODER_PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--decoder-first-pipeline-num-layers DECODER_FIRST_PIPELINE_NUM_LAYERS]
                       [--decoder-last-pipeline-num-layers DECODER_LAST_PIPELINE_NUM_LAYERS]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--overlap-grad-reduce]
                       [--defer-embedding-wgrad-compute]
                       [--wgrad-deferral-limit WGRAD_DEFERRAL_LIMIT]
                       [--no-align-grad-reduce]
                       [--ddp-bucket-size DDP_BUCKET_SIZE]
                       [--ddp-average-in-collective] [--overlap-param-gather]
                       [--overlap-param-gather-with-optimizer-step]
                       [--no-align-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--use-tp-pp-dp-mapping] [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--renormalize-blend-weights] [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HuggingFaceTokenizer,Llama2Tokenizer,TikTokenizer,NullTokenizer}]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--tiktoken-pattern TIKTOKEN_PATTERN]
                       [--tiktoken-num-special-tokens TIKTOKEN_NUM_SPECIAL_TOKENS]
                       [--tiktoken-special-tokens TIKTOKEN_SPECIAL_TOKENS [TIKTOKEN_SPECIAL_TOKENS ...]]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--num-dataset-builder-threads NUM_DATASET_BUILDER_THREADS]
                       [--s3-cache-path S3_CACHE_PATH] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS]
                       [--moe-shared-expert-intermediate-size MOE_SHARED_EXPERT_INTERMEDIATE_SIZE]
                       [--moe-shared-expert-overlap]
                       [--moe-router-load-balancing-type {aux_loss,sinkhorn,none}]
                       [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-pre-softmax] [--moe-grouped-gemm]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dispatcher-type {allgather,alltoall,alltoall_seq}]
                       [--moe-per-layer-logging]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-layer-recompute] [--moe-extended-tp]
                       [--moe-use-upcycling] [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK]
                       [--qk-head-dim QK_HEAD_DIM]
                       [--qk-pos-emb-head-dim QK_POS_EMB_HEAD_DIM]
                       [--v-head-dim V_HEAD_DIM]
                       [--rotary-scaling-factor ROTARY_SCALING_FACTOR]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR]
                       [--logging-level LOGGING_LEVEL] [--log-straggler]
                       [--disable-straggler-on-startup]
                       [--straggler-ctrlr-port STRAGGLER_CTRLR_PORT]
                       [--straggler-minmax-count STRAGGLER_MINMAX_COUNT]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--transformer-impl {local,transformer_engine}]
                       [--fp8-param-gather]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--hybrid-attention-ratio HYBRID_ATTENTION_RATIO]
                       [--hybrid-mlp-ratio HYBRID_MLP_RATIO]
                       [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN]
                       [--yaml-cfg YAML_CFG] [--no-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--one-logger-async]
                       [--app-tag-run-name APP_TAG_RUN_NAME]
                       [--app-tag-run-version APP_TAG_RUN_VERSION]
                       [--enable-ft-package]
                       [--config-logger-dir CONFIG_LOGGER_DIR]
pretrain_gpt.py: error: unrecognized arguments: --mock data
W1113 22:59:41.765741 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3516 closing signal SIGTERM
W1113 22:59:41.766361 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3517 closing signal SIGTERM
W1113 22:59:41.766480 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3518 closing signal SIGTERM
W1113 22:59:41.767147 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3520 closing signal SIGTERM
W1113 22:59:41.767232 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3521 closing signal SIGTERM
W1113 22:59:41.767308 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3522 closing signal SIGTERM
W1113 22:59:41.767899 139685672710784 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 3523 closing signal SIGTERM
E1113 22:59:41.932318 139685672710784 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 2) local_rank: 3 (pid: 3519) of binary: /opt/conda/envs/py_3.9/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/py_3.9/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.4.0.dev20240501+rocm6.1', 'console_scripts', 'torchrun')())
  File "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/distributed/run.py", line 900, in main
    run(args)
  File "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/distributed/run.py", line 891, in run
    elastic_launch(
  File "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-13_22:59:41
  host      : tw015.pit.tensorwave.lan
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 3519)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
throughput per GPU: nan
elapsed time per iteration: nan
tokens/GPU/s: nan
mem usages: nan
